# Paxos

This is a Paxos implementation based on R. Renesse's Paxos Made Moderately
Complex.


Project Information
==============================================================================
CS 380D Distributed Computing, Fall 2015

Project 2, Paxos
* Hangchen Yu, UT EID: HY4987, UTCS ID: ZHITINGZ
* Zhiting Zhu, UT EID: ZZ3883, UTCS ID: HYU


Testing the Project
==============================================================================
The source codes are put in `./src`. `COMMAND` file contains a single line
`./src/Master.py` which starts the master of Paxos system. Execute:
```
$ ./tester.py
```

It will test all the input files in `./tests`. The inputs must be named with
extension `.test`. It will compare the output of `Master.py` with the standard
outputs in `./solutions`.

`Python3` is required.


Some Assumptions
==============================================================================
* The tester should leave enough minutes for each test case. For the given test
cases, our program runs in less than 20 seconds. But it may run longer when
given other tests.

* There will be always at lease one replica alive in the system.

* If you want to tolerate `f` faults, the number of servers should be at least
`2f+1`.

* Each `restartServer` is followed by a `allClear` (actually, not necessary in
our project).

* Each `timeBombLeader` follows a `allClear`.

* `timeBombLeader` only counts the messages between distinct servers, even if
the receiver is dead.

* No `sendMessage` is input when the alive processes are less than half (`<=f`).

* You should make sure that the indexes are not out of bound.

* The channel is valid.


Modification of Paxos
==============================================================================

Master
------------------------------------------------------------------------------
The master reads the input commands, and send messages to clients or servers
accordingly. Besides, the master needs to create the processes of servers and
clients. All the output that tester needs are written by master (to stdout).

As there are some commands requiring blocking, we use asynchronous blocking
instead of `time.sleep()` to wait for the completion of these commands. At the
end of the execution of master, it kills all the processes in order to release
the ports occupied by them.

Server (Node)
------------------------------------------------------------------------------
The code for server lives in node.py. Each server may have multiple roles
(Acceptor, Replica, Leader, Scout and Commander). For Paxos to work, we need
to have `2f+1` acceptors and `f+1` replicas. In our implementation, we choose
to have 2f+1 replicas to better handling the case when f+1 replicas fail.
Each server calculates `f` and assign the role based on the node id. `0` .. `f`
are replicas and all servers are acceptors. Initially, master assign server 0 as
the leader and if it fails, server will elect a new leader according to the
leader election algorithm described below. We encapsulate each role as a
separate class and create a cooresponding object when a server needs a role to
perform Paxos operations.

The created objects include Replica, Leader and Acceptor. If a server is the
leader, then it also creates some Scouts and Commanders. When the server
receives a message, it throws the message to cooresponding object.

Client
------------------------------------------------------------------------------
The client sends messages to all replicas (numbered from `0`..`f`) in the
system. It stores the messages in a buffer, and remove the message from the
buffer when receiving `decision` of it. When the buffer is empty, the client
tells the master that the blocking of `allClear` can be cleared. When a client
hears that the election of new leader is completed, it will resend the messages
in the buffer.

According to the implementation, when client does not clear its message buffer
after long enough time, the system is stable (not changed meaningfully by the
time). Then client will return `allClear` to master directly, and move the
progress on.

In order to deal with the case where a sendMessage is executed when all
replicas fails, the client will notify the leader to reset the state
periodically when it is dealing with `allClear` from master. This operation
is aimed to reset the whole Paxos system.


Some Techniques
==============================================================================

Message Formats
------------------------------------------------------------------------------
The messages are stored as tuples. Typically, a message is in the following
format (we use the propose from Replicas as example):
```python
message = ('propose', (slot_num, proposal))
```

`message[0]` means the type of the message, and `message[1]` contains the
information. `slot_num` is an integer, and `proposal` is another tuple:
```python
proposal = (client_id, command_id, chatlog)
```

The messages are parsed by `ast.literal_eval()`.

Asynchronous Blocking
------------------------------------------------------------------------------
Master and Clients may be blocked due to `restartServer`, `printChatLog` and
`allClear`. We do not use `time.sleep()` to implement the blocking. Instead,
the processes wait for others' acknowledges to get out of the blocking.

Failure Detection
------------------------------------------------------------------------------
The only leader in the system keeps broadcasting heartbeat to other servers.
Each server creates a thread to detect the heartbeat. When a server does not
receive heartbeat in a fixed period (timeout), this server regards the leader
crashed, and starts a leader election protocol.

Leader Election
------------------------------------------------------------------------------
We made a bit modification of the leader election in Three-Phase-Commit project.
When a leader election starts, the server marks `leader_is_dead` as `True`,
regards itself as the new leader, and broadcast a heartbeat containing its
`node_id`. If a server receives a heartbeat from others, it updates its
`leader_id` to the `node_id` in the heartbeat when `node_id` smaller than its
own id or `leader_is_dead` is `True`. Then it marks `leader_is_dead` as `False`.

After three rounds (actually two are enough), all servers have the same
`leader_id`, which is the smallest index of them. Then this new leader sends a
acknowledge message to all Clients. The Clients then resend the
messages in their waiting queues.


Using Auto Grader
==============================================================================
Run "python tester.py" and check your output.

To add your own tests, create a .test file and a .sol file in the tests/ and solutions/ directories respectively.
You can check what your output was in the answers/ directory.

Running tests manually:

To run the sample tests, replace test_name with the name of the test and execute the following command:

cat tests/[test_name].test | $(cat COMMAND)

To automatically check the output:

cat tests/[test_name].test | $(cat COMMAND) > temp_output && diff -q temp_output tests/[test_name].sol

If your output matches the solution, NOTHING will be printed. Otherwise the lines that differ will be shown.
The output for the run of the test will also be stored in a file temp_output after running the second command.
